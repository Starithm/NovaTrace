{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:03:47.912603Z",
     "start_time": "2025-05-23T16:03:43.374183Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoConfig, logging\n",
    "from seqeval.metrics import classification_report\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204fc9d56d52216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:03:48.666649Z",
     "start_time": "2025-05-23T16:03:48.662102Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuned_version=\"./astrobert-ner-finetuned_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b48724e3cceca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:03:49.294646Z",
     "start_time": "2025-05-23T16:03:49.287312Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "circular_list = sub_cirs = range(21916, 21916+10)\n",
    "def read_bio_files(filepaths):\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            tokens, labels = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    tokens.append(splits[0])\n",
    "                    labels.append(splits[1])\n",
    "            samples.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdae6cea552068f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:03:50.167141Z",
     "start_time": "2025-05-23T16:03:50.153350Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "foldername = \"manual_annotation_1\"\n",
    "filepaths = [f\"./{foldername}/{cir}.bio\" for cir in circular_list]\n",
    "# print(filepaths)\n",
    "bio_file_data = read_bio_files(filepaths)\n",
    "train_samples = bio_file_data.copy()\n",
    "print(len(train_samples), train_samples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d498008dc4c048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:03:52.326545Z",
     "start_time": "2025-05-23T16:03:52.314187Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_labels = sorted(set(l for s in train_samples for l in s[\"ner_tags\"]))\n",
    "label2id = {l: i for i, l in enumerate(unique_labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "print(label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d469aa189e0000",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:04:28.014466Z",
     "start_time": "2025-05-23T16:04:27.985647Z"
    }
   },
   "outputs": [],
   "source": [
    "for sample in train_samples:\n",
    "    sample[\"labels\"] = [label2id[l] for l in sample[\"ner_tags\"]]\n",
    "\n",
    "dataset = Dataset.from_list(train_samples)\n",
    "print(len(dataset))\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306cd9e7a95e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:18:22.878170Z",
     "start_time": "2025-05-23T16:18:19.856449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "remote_model_path = 'kusha7/astrobert-gcn-tokenizer'\n",
    "hf_token=os.enviro[\"hf_token\"]\n",
    "config = AutoConfig.from_pretrained(remote_model_path, num_labels=num_labels, id2label=id2label, label2id=label2id,)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(remote_model_path, token=hf_token)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    remote_model_path, token=hf_token,  config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "# max_len = max(len(tokenizer.tokenize(\" \".join(ex[\"tokens\"]))) for ex in train_samples)\n",
    "# print(max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730b40164067416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512, padding=\"max_length\")\n",
    "    word_ids = tokenized.word_ids()\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"labels\"][word_idx])\n",
    "        else:\n",
    "            labels.append(example[\"labels\"][word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "logging.set_verbosity_info()\n",
    "dataset = dataset.map(tokenize_and_align_labels)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842658ef6360113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training config\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=3,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"no\",  # optional\n",
    "    save_strategy=\"epoch\",\n",
    "    # fp16=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52309ccccfcb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(fine_tuned_version)\n",
    "tokenizer.save_pretrained(fine_tuned_version)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "label_list = model.config.id2label\n",
    "def predict(text, max_length=512, stride=256):\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Tokenize long input with overlapping chunks\n",
    "    tokenized = tokenizer(\n",
    "        tokens,\n",
    "        return_tensors=\"pt\",\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(len(tokenized[\"input_ids\"])):\n",
    "        inputs = {k: v[i].unsqueeze(0).to(model.device) for k, v in tokenized.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]  # first element of tuple is logits\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is None or word_id == word_ids[idx - 1]:\n",
    "                continue\n",
    "            token = tokens[word_id]\n",
    "            label = id2label[preds[idx]]\n",
    "            all_predictions.append((token, label))\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee78d911b5b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_circulars(c_list):\n",
    "    json_circulars = []\n",
    "    for sub_cir in c_list:\n",
    "        with open('./archive.json/{}.json'.format(sub_cir), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            json_circulars.append(data)\n",
    "    return json_circulars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6abe04b21611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_test = [21916]\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(\"Labels in model:\", model.config.id2label)\n",
    "circulars = get_circulars(to_test)\n",
    "print(circulars)\n",
    "predictions = predict(circulars[0][\"body\"])\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edf7e8eb737cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForTokenClassification.from_pretrained('kusha7/astrobert-gcn-tokenizer')\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('kusha7/astrobert-gcn-tokenizer')\n",
    "print(\"Labels in model base:\", base_model.config.id2label)\n",
    "base_id2label = base_model.config.id2label\n",
    "def predict_with_base(text):\n",
    "    tokens = text.split()\n",
    "    inputs = base_tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model(**inputs)\n",
    "        logits = outputs[0]  # first element of tuple is logits\n",
    "\n",
    "    preds = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    return list(zip(tokens, [base_id2label[p] for p in preds]))\n",
    "base_preds = predict_with_base(circulars[0]['body'])\n",
    "print(base_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff1281eada5d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
