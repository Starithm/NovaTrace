{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664821895f5d409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:00:42.285536Z",
     "start_time": "2025-05-23T17:00:37.839446Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:13:20.651055Z",
     "start_time": "2025-05-23T17:13:20.641326Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_cirs = range(21916, 21916+50)\n",
    "dataset_manual_annotation_folder = \"dataset_manual_annotation_v1\"\n",
    "    #[21916, 21917,21923, 21924, 21930,21941,22115,22281,22942]\n",
    "#sub_cirs = [21917]\n",
    "print([c for c in sub_cirs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed219f4fcc6749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:13:21.819264Z",
     "start_time": "2025-05-23T17:13:21.810238Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_circulars():\n",
    "    json_circulars = []\n",
    "    for sub_cir in sub_cirs:\n",
    "        with open('./archive.json/{}.json'.format(sub_cir), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            json_circulars.append(data)\n",
    "    return json_circulars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3f169caecf937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:13:23.359686Z",
     "start_time": "2025-05-23T17:13:23.329032Z"
    }
   },
   "outputs": [],
   "source": [
    "output = get_circulars()\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec611a4430b67e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:13:28.046338Z",
     "start_time": "2025-05-23T17:13:24.991773Z"
    }
   },
   "outputs": [],
   "source": [
    "remote_model_path = 'kusha7/astrobert-gcn-tokenizer'\n",
    "hf_token=os.environ[\"hf_token\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(remote_model_path, token=hf_token)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"kusha7/astrobert-gcn-tokenizer\", token=hf_token\n",
    ")\n",
    "# ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb205f36d87fe6b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:13:28.650905Z",
     "start_time": "2025-05-23T17:13:28.067949Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "label_list = model.config.id2label\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "def merge_subwords(tokens):\n",
    "    merged = []\n",
    "    buffer = \"\"\n",
    "    buffer_start = None\n",
    "    buffer_end = None\n",
    "    buffer_label = None\n",
    "\n",
    "    for token_info in tokens:\n",
    "        token = token_info['token']\n",
    "        start = token_info['start']\n",
    "        end = token_info['end']\n",
    "        label = token_info['label']\n",
    "        if token == '[CLS]' or token == '[SEP]':\n",
    "            continue\n",
    "\n",
    "        if token.startswith('##'):\n",
    "            # It's a subword — merge with buffer\n",
    "            buffer += token[2:]\n",
    "            buffer_end = end\n",
    "        else:\n",
    "            # Commit previous buffer if exists\n",
    "            if buffer:\n",
    "                merged.append({\n",
    "                    \"token\": buffer,\n",
    "                    \"start\": buffer_start,\n",
    "                    \"end\": buffer_end,\n",
    "                    \"label\": buffer_label\n",
    "                })\n",
    "                buffer = \"\"\n",
    "                buffer_start = buffer_end = buffer_label = None\n",
    "\n",
    "            # Start new buffer\n",
    "            buffer = token\n",
    "            buffer_start = start\n",
    "            buffer_end = end\n",
    "            buffer_label = label\n",
    "\n",
    "    # Commit the final buffer\n",
    "    if buffer:\n",
    "        merged.append({\n",
    "            \"token\": buffer,\n",
    "            \"start\": buffer_start,\n",
    "            \"end\": buffer_end,\n",
    "            \"label\": buffer_label\n",
    "        })\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da78aae2d35ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:13:29.427156Z",
     "start_time": "2025-05-23T17:13:29.410145Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_att(cir):\n",
    "    text = cir[\"body\"]\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    # print(sentences)\n",
    "    sentence_level_tokenization = []\n",
    "    for sent in sentences:\n",
    "        # print(\"sent\", sent)\n",
    "        sentence_results = []\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "        # Move input tensors to device\n",
    "        inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "\n",
    "        logits = outputs.logits.cpu()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        offsets = offset_mapping[0].tolist()\n",
    "        for token, offset, pred_id in zip(tokens, offsets, predictions[0]):\n",
    "            label = label_list[pred_id.item()]\n",
    "            sentence_results.append({\n",
    "                \"token\": token,\n",
    "                \"start\": offset[0],\n",
    "                \"end\": offset[1],\n",
    "                \"label\": label\n",
    "            })\n",
    "        sentence_level_tokenization.append({\n",
    "            \"sent\": sent,\n",
    "            \"result\":merge_subwords(sentence_results)})\n",
    "        # Output includes all tokens — even those labeled \"O\"\n",
    "    return sentence_level_tokenization\n",
    "def generate_partial_sentence_level_bio_format(json_circulars):\n",
    "    new_list = []\n",
    "    for sub_cir in json_circulars:\n",
    "        new_obj = sub_cir.copy()\n",
    "        text = sub_cir[\"body\"]\n",
    "        new_obj[\"body\"] = text.replace(\"\\n\", \" \")\n",
    "        new_obj[\"new_pipeline_output\"] = get_att(new_obj)\n",
    "        new_list.append(new_obj)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8607e9e866be20d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:14:03.750400Z",
     "start_time": "2025-05-23T17:13:31.593652Z"
    }
   },
   "outputs": [],
   "source": [
    "output_bio_2 = generate_partial_sentence_level_bio_format(output)\n",
    "# print(output_bio_2[0][\"body\"])\n",
    "print(output_bio_2[0][\"new_pipeline_output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce18c1c4241a5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:17:02.140716Z",
     "start_time": "2025-05-23T17:17:02.036848Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def write_bio(output_path, cir):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        sentence_level_arr = cir[\"new_pipeline_output\"]\n",
    "        to_be_saved = []\n",
    "        #print(\"sentence_level_arr\", len(sentence_level_arr))\n",
    "        for sentence in sentence_level_arr:\n",
    "            results = sentence[\"result\"]\n",
    "            sent = sentence[\"sent\"]\n",
    "            #print(\"results\", results)\n",
    "            to_be_saved.append({\n",
    "                \"sentence\": sent,\n",
    "                \"list\": [{\"token\": r[\"token\"], \"label\":r[\"label\"]} for r in results]\n",
    "            })\n",
    "\n",
    "        final_json = json.dumps(to_be_saved, indent = 2)\n",
    "        f.write(final_json)\n",
    "\n",
    "def create_bio_files(outputs):\n",
    "    os.makedirs(dataset_manual_annotation_folder, exist_ok=True)\n",
    "    for bio_output in outputs:\n",
    "        # print(\"bio_output\", bio_output)\n",
    "        write_bio(f\"{dataset_manual_annotation_folder}/{bio_output['circularId']}.json\", bio_output)\n",
    "\n",
    "create_bio_files(output_bio_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0193aa8448df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
